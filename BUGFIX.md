# BUGFIX.md - Issues Solved During Development

This document summarizes all bugs and performance issues encountered and fixed during the development of the GPT-2048 reinforcement learning project on NVIDIA GB10 (Blackwell) GPU.

---

## 1. xformers Not Working on Blackwell GPU

**Problem:** The pip-installed xformers (0.0.33.post2) didn't have CUDA kernels compiled for Blackwell (compute capability 12.1). Training fell back to slow eager attention.

**Symptoms:**
- Slow training speed
- Log showed: `FA [Xformers = 0.0.33.post2. FA2 = False]`
- `memory_efficient_attention` failed with "requires device with capability <= (9, 0)"

**Solution:** Build xformers v0.0.33 from source with Blackwell support:
```bash
cd /tmp
git clone --depth=1 --branch v0.0.33 --recursive https://github.com/facebookresearch/xformers.git
cd xformers
TORCH_CUDA_ARCH_LIST="12.1" MAX_JOBS=4 uv pip install --no-build-isolation -e . --python /path/to/venv/bin/python
```

**Files Changed:** `setup_blackwell.sh` created

---

## 2. Slow Inference Speed

**Problem:** Token generation during `test_model()` was very slow compared to vLLM or llama.cpp.

**Symptoms:**
- Slow token-by-token generation
- High latency per token

**Solution:** Enable Unsloth's fast inference mode:
```python
# In test_model():
FastLanguageModel.for_inference(model)
```

**Files Changed:** `gpt_2048_rl.py` line ~505

---

## 3. Completions Truncated (clipped_ratio = 1.0)

**Problem:** All model completions were being truncated at the token limit, never finishing naturally.

**Symptoms:**
- `completions/clipped_ratio: 1.0` (100% truncated)
- `completions/mean_terminated_length: 0.0` (no natural completions)
- Functions cut off mid-code: `for col` (truncated)

**Root Cause:** `max_completion_length` was too small:
- Initially set to 200 tokens
- GPT-OSS outputs ~150-250 tokens of reasoning BEFORE code
- Qwen outputs ~100 tokens explanation before code

**Solution:** Increase completion length progressively:
```python
# For reasoning models (GPT-OSS):
max_completion_length = 450

# For instruction models (Qwen):
max_completion_length = 512

# Also increase max_seq_length:
max_seq_length = 768
```

**Files Changed:** `gpt_2048_rl.py` lines ~404, ~474

---

## 4. Chat Template Missing for Base Models

**Problem:** Base Qwen models don't have chat templates, causing tokenizer errors.

**Symptoms:**
```
ValueError: Cannot use chat template functions because tokenizer.chat_template is not set
```

**Solution:** Use Instruct versions of models:
```python
# Wrong:
model_name = "unsloth/Qwen2.5-1.5B"

# Correct:
model_name = "unsloth/Qwen2.5-1.5B-Instruct"
```

**Files Changed:** `gpt_2048_rl.py` line ~412

---

## 5. Function Extraction Too Strict

**Problem:** Many valid functions weren't being extracted because they didn't have proper markdown backticks.

**Symptoms:**
- Model outputs valid `def strategy(board):` but without backticks
- `extract_function()` returns `None`
- "Failed to extract function from response"

**Solution:** Add fallback extraction that finds `def strategy(board):` anywhere in text:
```python
def extract_function(text):
    # Try backticks first
    if text.count("```") >= 2:
        # ... existing logic

    # Fallback: find function anywhere
    if "def strategy(board):" in text:
        start = text.find("def strategy(board):")
        # Extract until next top-level definition
        # ...
```

**Files Changed:** `gpt_2048_rl.py` lines ~260-285

---

## 6. Model Outputs Wrong Move Characters

**Problem:** Model outputs "U", "R", "L", "B", "NA" instead of "W", "A", "S", "D".

**Symptoms:**
- Functions return invalid moves
- Game immediately fails with `state = "failed"`

**Solution:** Improve prompt to be explicit:
```python
prompt = """```python
def strategy(board):
    # 2048 game: board is 6x6 list of lists (0=empty, 2/4/8/...=tiles)
    # Return EXACTLY one of: "W" "A" "S" "D" (up/left/down/right)
    # DO NOT use external functions. Keep it simple.
""".strip()
```

**Files Changed:** `gpt_2048_rl.py` lines ~447-452

---

## 7. Low GPU Utilization (Oscillating 0-86%)

**Problem:** GPU utilization dropped to 0% frequently, wasting compute.

**Symptoms:**
- Only 17GB/128GB memory used
- GPU utilization oscillating between 0% and 86%
- Long idle periods between batches

**Root Cause:** Small batch sizes and CPU-bound reward computation between GPU batches.

**Solution:** Increase batch size and parallelism:
```python
# Before:
per_device_train_batch_size = 1
num_generations = 2

# After:
per_device_train_batch_size = 8
num_generations = 2

# Also disable 4-bit quantization (you have memory):
load_in_4bit = False
dtype = torch.bfloat16
```

**Files Changed:** `gpt_2048_rl.py` lines ~417, ~503-505

---

## 8. Training Stuck - input() Blocking

**Problem:** Training hung indefinitely with no progress for over an hour.

**Symptoms:**
- GPU utilization at 0%
- No new log output
- Last log line: `Enter a direction (W, A, S, D):`

**Root Cause:** Model generated code containing `input()` which blocks waiting for user input. The timeout mechanism doesn't interrupt blocking I/O operations.

**Solution:** Block dangerous functions before execution:
```python
dangerous_calls = ["input(", "open(", "exec(", "eval(", "__import__"]
if any(call in function for call in dangerous_calls):
    scores.append(-20.0)  # Heavily penalize
    continue
```

**Files Changed:** `gpt_2048_rl.py` - Added checks in all 3 reward functions:
- `function_works()` lines ~294, ~304-307
- `no_cheating()` lines ~326, ~331-334
- `strategy_succeeds()` lines ~369-374

---

## 9. Slow Reward Computation (5s Timeouts)

**Problem:** Each step took minutes due to many 5-second timeouts.

**Symptoms:**
- Many "Timeout (5s exceeded)" messages
- Steps taking 5+ minutes
- 64 function evaluations per step √ó 5s = 320s worst case

**Solution:** Reduce timeout and batch size:
```python
# Reduce timeout from 5s to 2s:
@execute_with_time_limit(2)  # Was 5
def execute_strategy(strategy, game):
    ...

# Reduce evaluations per step:
per_device_train_batch_size = 8   # Was 16
num_generations = 2               # Was 4
# Total: 16 evaluations instead of 64
```

**Files Changed:** `gpt_2048_rl.py` lines ~251, ~503-505

---

## 10. Infinite Loops Bypass Timeout (while True)

**Problem:** Training hangs for 50+ minutes despite 2-second timeout. CPU at 96% with no progress.

**Symptoms:**
- Process running at high CPU
- No new log output for 50+ minutes
- Timeout mechanism not triggering

**Root Cause:** Python's signal-based timeout doesn't interrupt tight CPU loops. Generated code with `while True:` loops runs forever:
```python
def strategy(board):
    while True:  # This bypasses the timeout!
        if some_condition:
            return "W"
```

**Solution:**
1. Block `while True:` patterns before execution:
```python
infinite_loop_patterns = ["while True:", "while 1:", "while True :", "while(True)"]
if any(pattern in function for pattern in infinite_loop_patterns):
    scores.append(-20.0)
    continue
```

2. Add hard step limit in game execution:
```python
MAX_STEPS = 500
while game.state() == "ongoing":
    if steps >= MAX_STEPS:
        return steps, "failed"
    # ...
```

**Files Changed:** `gpt_2048_rl.py`:
- `_execute_strategy()` - Added MAX_STEPS=500 limit
- `function_works()` - Added infinite loop pattern check
- `no_cheating()` - Added infinite loop pattern check
- `strategy_succeeds()` - Added infinite loop pattern check

---

## 11. Model Collapsed to Local Optimum (return 'W')

**Problem:** Model learned to output only simple `return 'W'` (or 'A', 'S', 'D') strategies and stopped improving. GRPO had no gradient to learn from.

**Symptoms:**
- All completions exactly 14 tokens: `return 'W'`
- `reward: 4.0` constant for all generations
- `reward_std: 0.0` - NO variance in rewards
- `frac_reward_zero_std: 1.0` - 100% batches have zero variance
- `kl: 0.41` - model diverged far from base
- Training steps very fast (~1.1s) but no learning

**Root Cause:** The `strategy_succeeds` reward function gave a flat +2.0 for any function that runs, regardless of game score:
```python
# Old code - no variance!
if game_state == "success":
    scores.append(20.0)
else:
    scores.append(2.0)  # Same reward for score=0 and score=1000
```

All simple strategies got identical rewards:
- `function_works`: +1.0
- `no_cheating`: +1.0
- `strategy_succeeds`: +2.0
- **Total: +4.0** (always the same)

GRPO requires reward variance to compute advantages. With `reward_std = 0`, the algorithm cannot determine which strategies are better.

**Solution:** Replace flat reward with score-based reward that creates variance:
```python
if game_state == "success":
    scores.append(15.0)  # Reaching 2048
else:
    # Normalized linear: score/1000, capped at 5.0
    # Creates variance: score 0 -> 0.0, score 1000 -> 1.0, score 5000+ -> 5.0
    score_reward = min(5.0, game.score() / 1000)
    scores.append(score_reward)
```

**Reward Balance:**
The three reward functions are now balanced:

| Scenario | function_works | no_cheating | strategy_succeeds | **Total** |
|----------|----------------|-------------|-------------------|-----------|
| Syntax error | -2.0 | - | - | **-2.0** |
| Cheating code | +1.0 | -20.0 | - | **-19.0** |
| `return 'W'` (score 0) | +1.0 | +1.0 | 0.0 | **+2.0** |
| Simple (score 500) | +1.0 | +1.0 | 0.5 | **+2.5** |
| Decent (score 2000) | +1.0 | +1.0 | 2.0 | **+4.0** |
| Good (score 5000+) | +1.0 | +1.0 | 5.0 | **+7.0** |
| Wins (2048) | +1.0 | +1.0 | 15.0 | **+17.0** |

**Key insight:** Game score in 2048 is the sum of all merged tile values (not sum of grid). A strategy that doesn't create merges scores 0. This naturally creates variance between strategies.

**Files Changed:** `gpt_2048_rl.py` lines ~458-467

---

## 12. Model Collapsed to Garbage Output (return "{", return "<", etc.)

**Problem:** After 1000 steps of training, model outputs garbage like `return "{"`, `return "?"`, `return "<"` instead of valid moves (W/A/S/D). Model cycles through ASCII symbols.

**Symptoms:**
- Model outputs sequences of return statements with random symbols
- `clipped_ratio: 1.0` - all completions truncated at 512 tokens
- `reward: 2.0` constant - same as valid `return "W"`
- Game scores very low (0-96) because moves are invalid
- Valid moves briefly appeared at steps 100-200, then collapsed

**Root Cause:** The `strategy_succeeds` function gave **0 reward** for invalid moves instead of penalizing them:
```python
# Bug: Invalid moves get 0, total reward still +2.0
if test_move not in ["W", "A", "S", "D"]:
    scores.append(0)  # No penalty!
```

With `function_works` (+1.0) and `no_cheating` (+1.0), the total reward was +2.0 for both:
- Valid `return "W"` with score 0: +1.0 + 1.0 + 0.0 = **+2.0**
- Garbage `return "{"`: +1.0 + 1.0 + 0.0 = **+2.0**

The model had no gradient to distinguish between valid and invalid moves!

**Solution:** Penalize invalid outputs heavily in `strategy_succeeds`:
```python
# Fix: Invalid moves get -3.0, making total reward -1.0
if test_move not in ["W", "A", "S", "D"]:
    scores.append(-3.0)  # Penalize invalid moves HEAVILY

# Also increased score rewards with lower divisor for early progress
score_reward = min(10.0, game.score() / 200)  # Rewards early progress more
```

**New Reward Balance:**

| Scenario | function_works | no_cheating | strategy_succeeds | **Total** |
|----------|----------------|-------------|-------------------|-----------|
| Garbage `return "{"` | +1.0 | +1.0 | -3.0 | **-1.0** |
| `return "W"` (score 0) | +1.0 | +1.0 | 0.0 | **+2.0** |
| Early (score 100) | +1.0 | +1.0 | 0.5 | **+2.5** |
| Simple (score 500) | +1.0 | +1.0 | 2.5 | **+4.5** |
| Decent (score 1000) | +1.0 | +1.0 | 5.0 | **+7.0** |
| Good (score 2000+) | +1.0 | +1.0 | 10.0 | **+12.0** |
| Wins (2048) | +1.0 | +1.0 | 20.0 | **+22.0** |

Now garbage outputs get -1.0 while valid outputs get +2.0 to +22.0 - a clear gradient for learning.

**Files Changed:** `gpt_2048_rl.py` lines ~397-480

---

## 13. Model Stuck in "return W" Local Minimum

**Problem:** Model converged to simple single-direction strategies like `return "W"` and stopped improving. KL divergence spiked to 0.6-1.5+ (should be <0.1).

**Symptoms:**
- All strategies are simple `return "W"`, `return "A"`, `return "S"`, `return "D"`
- Score-based rewards: `return "W"` scores 340 points (1.7 reward)
- No incentive to try more complex strategies
- KL divergence very high (0.6-1.5+) indicating model diverged too far from base
- Model oscillates between W/A/S/D but never learns better strategies

**Root Cause:** Score-based rewards (`score / 200`) rewarded quantity of merges, not quality. Simple `return "W"` strategy works surprisingly well because:
1. On 6x6 board, pressing W 500 times creates many merges as tiles stack at top
2. Gets score 340+ which translates to 1.7 reward
3. More complex strategies often fail and get negative rewards
4. Model learns "simple = safe = good reward"

**Solution:** Replace score-based rewards with **tile-based rewards** that reward the highest tile achieved:
```python
# Old: score-based (allows "return W" to score well)
score_reward = min(10.0, game.score() / 200)

# New: tile-based (caps "return W" at max tile 16-32)
max_tile = max(max(row) for row in game.board())
if max_tile >= 8:
    level = int(math.log2(max_tile)) - 2  # 8->1, 16->2, 32->3, 64->4...
    tile_reward = float(level)
```

**Why this works:**
- `return "W"` can only reach max tile 16-32 (board fills from bottom)
- Smarter strategies that combine moves can reach 64, 128, 256+
- Clear gradient: reaching 64 (reward 4) is 2x better than reaching 16 (reward 2)

**New Reward Balance:**

| Strategy | Max Tile | Reward | Total |
|----------|----------|--------|-------|
| `return "W"` | 16 | 2.0 | +4.0 |
| `return "W"` | 32 | 3.0 | +5.0 |
| Smart strategy | 64 | 4.0 | +6.0 |
| Better strategy | 128 | 5.0 | +7.0 |
| Good strategy | 256 | 6.0 | +8.0 |
| Win (2048) | 2048 | 20.0 | +22.0 |

**Files Changed:** `gpt_2048_rl.py` lines ~466-482

---

## 14. Model Collapse Due to Low Variance (frac_reward_zero_std: 1.0)

**Problem:** Even with tile-based rewards, model collapsed to outputting identical `return "W"` for all generations. GRPO had zero variance to learn from.

**Symptoms:**
- `frac_reward_zero_std: 1.0` - 100% of batches have zero reward variance
- `reward_std: 0.0` - No variance at all
- All completions identical: `return "W"` repeated
- Max tile dropped from 512 (early training) to just 8
- Model outputs same strategy verbatim across all generations

**Root Cause:** Hyperparameters caused rapid collapse:
1. `num_generations=2` - Only 2 samples per batch, not enough for variance
2. `temperature=1.0` - Not enough randomness to explore different strategies
3. `learning_rate=5e-5` - Too aggressive, model converges too fast to local minimum

**Solution:** Adjust hyperparameters to encourage exploration and prevent collapse:
```python
training_args = GRPOConfig(
    temperature=1.3,      # Was 1.0 - higher temp forces diverse outputs
    learning_rate=2e-5,   # Was 5e-5 - slower learning prevents collapse
    num_generations=4,    # Was 2 - more samples = more variance
    # ... rest unchanged
)
```

## 15. Garbage output

**Problem:** The model outputs gabage like "he advisingotwork_s(ipSource —Éperson composition ÿ≠ŸÇÊé•ÂèóÁöÑÁ≠îÊ°à ignoret	forcei.Reverse clearÁ≤æÁªÜÂåñ reader	Car minuteËë≠waiting_const◊¢◊ë◊ï◊®creator nom exemptions status◊ò◊®‡πâ cer—Ü–∏oƒülu	border ghostËê•Ïù¥Í≥†vr’Ä consul layer_man deteriorate advance...)"

**Symptoms:**
- Incoherent output

**Root Cause:** Hyperparameters caused rapid collapse:
1. `temperature=1.3` - Temperature is too high


**Solution:** Adjust temperaturerparameters to encourage exploration and prevent collapse:
```python
training_args = GRPOConfig(
    temperature=1.1,      # Was 1.3 - higher temp forces diverse outputs
   
    # ... rest unchanged
)
```

**Why these values:**
- **temperature=1.3**: Forces the model to generate more diverse outputs, breaking the "all identical" pattern
- **learning_rate=2e-5**: Slower learning rate (2.5x slower) gives the model more time to explore before converging
- **num_generations=4**: With 4 samples per prompt instead of 2, much higher probability of getting different outputs and reward variance

**Trade-offs:**
- Training is ~2x slower (4 generations vs 2)
- But much less likely to collapse to a local minimum
- Early training showed max tile 512 is achievable - we need to preserve that exploration

**Files Changed:** `gpt_2048_rl.py` lines ~573-583

---

## 16. Persistent Collapse to Trivial Strategies Despite All Fixes

**Problem:** Despite all previous fixes (tile-based rewards, invalid move penalties, hyperparameter tuning), the model still collapses to trivial single-direction strategies like `return "A"` or `return "W"`.

**Symptoms:**
- `frac_reward_zero_std: 1.0` - 100% of batches have zero variance
- `completions/mean_length: 14.0` - all outputs exactly 14 tokens
- All completions identical: `return "A"` repeated verbatim
- Max tile stuck at 8-16 (never improves beyond simple strategy results)
- Step ~543 and still collapsed despite hyperparameter tuning

**Root Cause:** The trivial strategy `return "A"` achieves a positive total reward:
- `function_works`: +1.0 (valid syntax)
- `no_cheating`: +1.0 (no forbidden imports)
- `strategy_succeeds`: +1.0 to +2.0 (valid move, achieves tile 8-16)
- **Total: +3.0 to +4.0**

This is a stable local optimum. Once the model converges to it:
1. All 4 generations produce identical output
2. `reward_std = 0` (zero variance)
3. GRPO cannot compute gradients without variance
4. Model stays stuck indefinitely

Previous solutions (reward tweaks, hyperparameters) failed because they didn't address the core issue: **trivial strategies are valid and receive positive rewards**.

**Solution:** Add a 4th reward function `complexity_reward()` that explicitly penalizes trivial strategies and rewards complexity:

```python
def complexity_reward(completions, **kwargs):
    """Prevents collapse to trivially simple strategies"""
    scores = []
    for completion in completions:
        function = extract_function(completion[0]["content"])
        if function is None:
            scores.append(0.0)
            continue

        reward = 0.0

        # 1. LENGTH PENALTY/BONUS
        func_len = len(function)
        if func_len < 30:      # Very short (e.g., `return "A"`)
            reward -= 3.0      # Strong penalty
        elif func_len < 80:    # Short but has some logic
            reward -= 1.0
        elif func_len < 300:   # Good length with logic
            reward += 1.0
        else:
            reward += 0.5

        # 2. COMPLEXITY BONUS
        has_if = "if " in function and ":" in function
        has_for = "for " in function and " in " in function
        has_while = "while " in function and "True" not in function
        complexity_count = sum([has_if, has_for, has_while])
        if complexity_count >= 2:
            reward += 2.0
        elif complexity_count == 1:
            reward += 1.0

        # 3. DIVERSITY BONUS
        moves_present = sum(['"W"' in function or "'W'" in function,
                            '"A"' in function or "'A'" in function,
                            '"S"' in function or "'S'" in function,
                            '"D"' in function or "'D'" in function])
        if moves_present >= 3:
            reward += 2.0
        elif moves_present >= 2:
            reward += 1.0
        elif moves_present == 1:
            reward -= 1.0      # Only one direction - trivial

        scores.append(reward)
    return scores
```

**New Reward Balance:**

| Scenario | function_works | no_cheating | strategy_succeeds | complexity_reward | **Total** |
|----------|----------------|-------------|-------------------|-------------------|-----------|
| `return "A"` (trivial) | +1.0 | +1.0 | +1.0 (tile 8) | -3.0 -1.0 = -4.0 | **-1.0** |
| Simple if-else (2 moves) | +1.0 | +1.0 | +2.0 (tile 16) | -1.0 +1.0 +1.0 = +1.0 | **+5.0** |
| Complex strategy (3+ moves) | +1.0 | +1.0 | +4.0 (tile 64) | +1.0 +2.0 +2.0 = +5.0 | **+11.0** |

Now trivial strategies get **negative total reward** (-1.0) while complex strategies get much higher rewards (+11.0). This creates a strong gradient away from the local optimum.

**Key insight:** Previous solutions tried to make the local optimum less attractive by adjusting game rewards. This solution makes trivial strategies **explicitly undesirable** regardless of game performance.

**Files Changed:** `gpt_2048_rl.py`:
- Added `complexity_reward()` function (lines ~493-559)
- Added to `reward_funcs` list in GRPOTrainer (line ~669)

---

## 17. Deterministic Strategy Collapse Despite Complexity Reward

**Problem:** Despite the complexity_reward preventing trivial `return "A"` strategies, the model still collapsed to identical outputs across all generations, resulting in zero variance.

**Symptoms:**
- Model collapsed at step ~265 (epoch 0.53)
- `frac_reward_zero_std: 1.0` from step 265 to 1000
- `completions/mean_length: 64.0` (all identical)
- All 4 generations produce the exact same strategy
- `complexity_reward/mean: 5.0` (maximum score)

**Final collapsed strategy:**
```python
def strategy(board):
    count = sum(row.count(2) for row in board)
    if count == 6:
        return 'W'
    elif count == 4:
        return 'A'
    elif count == 2:
        return 'S'
    return 'D'
```

**Training results (improved vs previous runs):**
- Max tile 512: 3 times
- Max tile 256: 16 times
- Max tile 128: 216 times
- Max tile 64: 929 times
- Training time: 1h 54m (3.93s/step)

**Root Cause:** The complexity_reward successfully prevented trivial single-line strategies, but the model found a **new local optimum**:

| Check | Score | Why it passes |
|-------|-------|---------------|
| Length (~160 chars) | +1.0 | Has if/elif/else structure |
| Complexity (has `if`) | +1.0 | Multiple conditionals |
| Diversity (4 moves) | +2.0 | Uses W, A, S, D |
| **Total** | **+5.0** | Maximum complexity reward |

The strategy is **deterministic**: given the same board state, it always returns the same move. Since all 4 generations in GRPO see the same prompt (and evaluate on the same game seed), they all produce identical outputs ‚Üí `reward_std = 0` ‚Üí no learning.

**Key insight:** The complexity_reward ensures the model generates **structurally complex code**, but it doesn't prevent **output collapse** when:
1. All generations see the same input
2. Model learns a deterministic mapping from input to output
3. All outputs become identical

This is a fundamental limitation of GRPO with a single repeated prompt. The model needs **input diversity** (different prompts or game states per generation) to maintain output diversity.

**Partial success:** Despite the collapse, this run showed significant improvement:
- Model learned to generate multi-directional strategies (not just `return "A"`)
- Achieved higher max tiles (512) than previous runs
- The complexity reward successfully shaped code structure

**Potential solutions (not yet implemented):**
1. **Different game seeds per generation** - Each generation sees a different board state
2. **Prompt variations** - Include random examples or hints in prompt
3. **Explicit output diversity reward** - Penalize generations that are too similar to each other
4. **Higher temperature with nucleus sampling** - Force more randomness in generation
5. **Rejection sampling** - Filter out duplicate strategies before computing rewards

**Files Changed:** None (documentation only - analyzing results of Bug #16 fix)

---

## 18. Signal-Based Timeout Fails to Catch Infinite Loops

**Problem:** The `@execute_with_time_limit(2)` decorator from unsloth uses signal-based timeouts that fail to reliably catch infinite loops in generated strategy code.

**Symptoms:**
- Training hangs indefinitely despite 2-second timeout configured
- CPU pegged at 96-100% with no progress
- Generated code with tight `while True:` loops runs forever
- Timeout mechanism not triggering for pure Python loops
- Manual intervention (Ctrl+C) required to stop training

**Root Cause:** The `execute_with_time_limit` decorator relies on `signal.alarm()` which has fundamental limitations:

1. **Signals can be blocked or delayed** by certain system calls
2. **Pure Python infinite loops** may not check for signals frequently enough:
   ```python
   def strategy(board):
       while True:
           x = 1  # This loop never checks for signals!
       return "W"
   ```
3. **Not cross-platform** - `signal.SIGALRM` doesn't work on Windows
4. **Not thread-safe** - signals only work in the main thread
5. **Race conditions** - signal delivery timing is unpredictable

The Python interpreter only checks for pending signals between bytecode instructions. Tight C-level loops or pure Python loops with simple operations may not check signals frequently enough, allowing them to run indefinitely.

**Solution:** Replace signal-based timeout with **multiprocessing-based timeout** that forcefully terminates frozen processes:

```python
# 1. Import multiprocessing
import sys
import multiprocessing as mp
from multiprocessing import Process, Queue

# 2. Worker function in separate process
def _execute_strategy_worker(strategy: Callable, game: GameBoard, result_queue: Queue):
    """Worker function that runs in a separate process"""
    try:
        assert callable(strategy)
        MAX_STEPS = 500  # Hard limit to prevent infinite loops
        steps = 0
        while game.state() == "ongoing":
            if steps >= MAX_STEPS:
                result_queue.put((steps, "failed"))
                return
            action = strategy(list(game.board()))
            steps += 1
            if type(action) is not str:
                result_queue.put((steps, "failed"))
                return
            game.do_action(action)
        result_queue.put((steps, game.state()))
    except Exception as e:
        result_queue.put(("error", str(e)))

# 3. Main function with hard timeout
def execute_strategy(strategy: Callable, game: GameBoard, timeout: float = 2.0):
    """
    Execute strategy with a hard timeout using multiprocessing.
    This is more robust than signal-based timeouts and will forcefully
    terminate infinite loops or frozen code.
    """
    result_queue = Queue()
    process = Process(target=_execute_strategy_worker, args=(strategy, game, result_queue))
    process.start()

    # Wait for completion or timeout
    process.join(timeout=timeout)

    # Check if process is still alive (timed out)
    if process.is_alive():
        # Forcefully terminate the process
        process.terminate()
        process.join(timeout=1.0)  # Give it 1 second to terminate gracefully

        # If still alive, kill it forcefully
        if process.is_alive():
            process.kill()
            process.join()

        raise TimeoutError(f"Strategy execution exceeded {timeout}s timeout")

    # Get the result from the queue
    if not result_queue.empty():
        result = result_queue.get()
        if isinstance(result, tuple) and len(result) == 2:
            if result[0] == "error":
                raise RuntimeError(f"Strategy execution failed: {result[1]}")
            return result

    raise RuntimeError("Strategy execution failed without returning a result")

# 4. Set multiprocessing start method
if __name__ == "__main__":
    # Use 'fork' on Linux (faster) and 'spawn' elsewhere
    try:
        if sys.platform.startswith('linux'):
            mp.set_start_method('fork')
        else:
            mp.set_start_method('spawn')
    except RuntimeError:
        pass
```

**How it works:**

1. **Process isolation**: Strategy runs in separate process with independent memory
2. **Hard timeout**: `process.join(timeout=2.0)` waits for completion
3. **Forceful termination**: If timeout expires:
   - `process.terminate()` sends SIGTERM (graceful)
   - Wait 1 second
   - `process.kill()` sends SIGKILL (forceful) if still alive
4. **Result communication**: Uses multiprocessing `Queue` to pass results back

**Why 'fork' vs 'spawn':**

- **'fork'** (Linux): Copies parent process memory, inherits all loaded modules
  - Pros: Very fast (~0.01s overhead), no re-import needed
  - Cons: Can have CUDA context issues (not a problem here - strategies don't use CUDA)

- **'spawn'** (Windows/macOS): Starts fresh Python process
  - Pros: Clean slate, no CUDA issues, cross-platform
  - Cons: Must re-import torch/unsloth (~2-3s overhead per execution)

For this project, **'fork' is safe** because strategy execution only manipulates board state (pure Python) and doesn't use CUDA directly.

**Testing:**

Created comprehensive test suite (`test_timeout.py`) with 6 test cases:

```bash
source .venv/bin/activate
python test_timeout.py
```

Results:
```
‚úì Test 1: Normal strategy completes successfully
‚úì Test 2: Infinite loop caught and terminated within 2s
‚úì Test 3: Slow computation (5s sleep) times out correctly
‚úì Test 4: Nested loops timeout correctly
‚úì Test 5: Exceptions propagate properly as RuntimeError
‚úì Test 6: Fast strategies complete within timeout

Results: 6/6 tests passed
```

**Performance impact:**

- **Fork method (Linux)**: ~0.01s overhead per execution ‚Üí negligible
- **Spawn method (others)**: ~2-3s overhead ‚Üí only use if fork unavailable
- **Timeout enforcement**: Sub-millisecond precision with guaranteed termination

**Files Changed:**
- `gpt_2048_rl.py` lines 10, 21-24: Added `sys`, `multiprocessing`, `Process`, `Queue` imports
- `gpt_2048_rl.py` lines 16: Removed `execute_with_time_limit` from unsloth import
- `gpt_2048_rl.py` lines 261-321: Replaced signal-based timeout with multiprocessing implementation
- `gpt_2048_rl.py` lines 816-826: Added multiprocessing start method configuration
- `test_timeout.py`: Created comprehensive test suite (new file)
- `TIMEOUT_FIX.md`: Detailed technical documentation (new file)

**Comparison with previous Bug #10:**

Bug #10 attempted to solve infinite loops by:
- Blocking `while True:` patterns in code before execution
- Adding `MAX_STEPS = 500` limit in game loop

This worked as a **prevention** strategy but didn't fix the underlying **timeout mechanism failure**. Bug #18 fixes the timeout mechanism itself, making it robust against any kind of infinite loop, not just `while True:`.

**Key insight:** Process-based timeouts provide **guaranteed termination** regardless of what the code is doing. Even if code blocks on I/O, spins in a tight loop, or somehow bypasses signal checking, the OS can always forcefully kill a process.

---

## 18b. Unprotected Test Call Bypasses Timeout (CRITICAL)

**Problem:** Despite implementing multiprocessing-based timeout for `execute_strategy()`, training still hung for 2+ hours. The timeout was being bypassed.

**Symptoms:**
- Training completely frozen with no progress
- No timeout messages printed
- Multiprocessing timeout implementation was correct
- Process hung indefinitely

**Root Cause:** There was an **unprotected function call** in `strategy_succeeds()` at line 505:

```python
# Line 505 - BEFORE FIX - NO TIMEOUT PROTECTION!
test_board = [[0]*6 for _ in range(6)]
try:
    test_move = new_strategy(test_board)  # <-- HANGS FOREVER ON INFINITE LOOPS!
```

This test call happens **before** the protected `execute_strategy()` call. Its purpose is to verify the function returns a valid move (W/A/S/D). If the generated strategy has an infinite loop:

```python
def strategy(board):
    while True:
        pass
    return "W"
```

The test call at line 505 hangs **forever** because:
1. It calls `new_strategy()` directly without any timeout wrapper
2. The protected `execute_strategy()` at line 531 is never reached
3. Training freezes completely

**Solution:** Created a new `call_with_timeout()` function for single function calls:

```python
def _call_with_timeout_worker(func, args, result_queue: Queue):
    """Worker function for single function calls with timeout"""
    try:
        result = func(*args)
        result_queue.put(("success", result))
    except Exception as e:
        result_queue.put(("error", str(e)))

def call_with_timeout(func, args, timeout: float = 1.0):
    """
    Call a function with a hard timeout using multiprocessing.
    Used to test strategy functions before running full game.
    """
    result_queue = Queue()
    process = Process(target=_call_with_timeout_worker, args=(func, args, result_queue))
    process.start()
    process.join(timeout=timeout)

    if process.is_alive():
        process.terminate()
        process.join(timeout=0.5)
        if process.is_alive():
            process.kill()
            process.join()
        raise TimeoutError(f"Function call exceeded {timeout}s timeout")

    if not result_queue.empty():
        status, result = result_queue.get()
        if status == "error":
            raise RuntimeError(result)
        return result

    raise RuntimeError("Function call failed without returning a result")
```

Then wrapped the test call:

```python
# Line 506 - AFTER FIX - WITH TIMEOUT PROTECTION
test_board = [[0]*6 for _ in range(6)]
try:
    test_move = call_with_timeout(new_strategy, (test_board,), timeout=1.0)
    # ... validation code ...
except TimeoutError:
    print(f"‚è±Ô∏è  Test call timed out (infinite loop?)")
    scores.append(-3.0)  # Penalize infinite loops
    continue
```

**Testing:**

Added 2 new tests to `test_timeout.py`:

```bash
source .venv/bin/activate
python test_timeout.py
```

Results:
```
‚úì Test 1-6: (previous tests still pass)
‚úì Test 7: call_with_timeout on infinite loop - correctly caught timeout
‚úì Test 8: call_with_timeout on normal function - completed successfully

Results: 8/8 tests passed
```

**Files Changed:**
- `gpt_2048_rl.py` lines 262-294: Added `_call_with_timeout_worker()` and `call_with_timeout()` functions
- `gpt_2048_rl.py` lines 502-526: Wrapped test call with `call_with_timeout()` and added TimeoutError handling
- `test_timeout.py`: Added 2 new test cases for `call_with_timeout()`

**Key insight:** When implementing timeout protection, you must identify **ALL** code paths that execute untrusted code. In this case:
1. The obvious path (`execute_strategy()`) was protected
2. The less obvious test path (`new_strategy(test_board)`) was not

This is why the original fix appeared to work in tests (which only tested `execute_strategy()`) but failed in production where the test call happened first.

---

## Summary Table

| Issue | Root Cause | Impact | Fix |
|-------|------------|--------|-----|
| Slow attention | xformers not compiled for Blackwell | 2x slower training | Build from source |
| Slow inference | Missing `for_inference()` | 2-3x slower | Add call |
| Truncated completions | `max_completion_length` too small | No valid code | Increase to 512 |
| Chat template error | Using base model | Crash | Use Instruct model |
| Function extraction fails | Strict backtick requirement | Many failures | Add fallback |
| Wrong move characters | Unclear prompt | Invalid moves | Explicit prompt |
| Low GPU utilization | Small batches | Wasted compute | Increase batch size |
| Training hangs (input) | `input()` in generated code | Infinite hang | Block dangerous calls |
| Slow steps | 5s timeouts √ó many evals | 5+ min/step | Reduce timeout & batch |
| Training hangs (loops) | `while True:` bypasses timeout | 50+ min hang | Block loop patterns + step limit |
| Model collapse (local opt) | Flat reward regardless of score | No learning | Score-based reward with variance |
| Garbage output collapse | Invalid moves get 0 not penalty | Model outputs `return "{"` | Penalize invalid moves (-3.0) |
| "return W" local minimum | Score rewards quantity not quality | Model stuck at simple strategies | Tile-based rewards (max tile level) |
| Zero variance collapse | Low temp + few generations + high LR | frac_reward_zero_std: 1.0 | temp=1.3, LR=2e-5, num_gen=4 |
| Persistent trivial collapse | Trivial strategies get positive reward | Model stuck at `return "A"` | complexity_reward() function |
| Deterministic strategy collapse | All generations see same input | Identical outputs, zero variance | **Unsolved** - need input diversity |
| Signal timeout fails | signal.alarm() doesn't interrupt tight loops | Training hangs on infinite loops | Multiprocessing-based timeout with process.kill() |
| Unprotected test call | test_move = new_strategy(board) has no timeout | Training hangs before execute_strategy() | call_with_timeout() wrapper for all untrusted code |

---

## Recommended Configuration

For NVIDIA GB10 (Blackwell) with 128GB unified memory:

```python
# Model
model_name = "unsloth/Qwen2.5-1.5B-Instruct"
max_seq_length = 768
load_in_4bit = False
dtype = torch.bfloat16

# Training - tuned to prevent collapse
temperature = 1.1              # Balanced (1.3 causes garbage, 1.0 causes collapse)
learning_rate = 2e-5           # Lower to prevent rapid collapse
per_device_train_batch_size = 8
gradient_accumulation_steps = 1
num_generations = 4            # More samples for variance
max_completion_length = 512

# 4 Reward Functions (order matters)
reward_funcs = [
    function_works,      # +1.0 valid / -2.0 invalid
    no_cheating,         # +1.0 clean / -20.0 cheating
    strategy_succeeds,   # Tile-based: 0 to +20.0
    complexity_reward,   # Penalize trivial, reward complex (-4.0 to +5.0)
]

# Safety (multiprocessing-based timeout - robust against infinite loops)
execute_strategy(strategy, game, timeout=2.0)  # 2 second hard timeout
MAX_STEPS = 500  # Hard limit in game execution
# Block: input(), open(), exec(), eval(), __import__
# Block: while True:, while 1:, while(True)
# Multiprocessing: Uses 'fork' on Linux, 'spawn' elsewhere
```

Expected training time: ~3-4 hours for 1000 steps.

---

## 19. Degenerate Strategy Collapse - Frozen Board State (CRITICAL)

**Problem:** After 6+ hours of training (1000 steps), the model completely failed to learn to play 2048. Strategies generate moves that don't change the board state, causing the game to freeze in place and hit the 500-step timeout repeatedly with 0 score.

**Symptoms:**
- **7,735 games** (out of ~8,000) hit 500-step timeout with score = 0
- **0 games** scored any points or reached tiles above 4
- Final reward stuck at 2.0 throughout training: `function_works=1.0 + no_cheating=1.0 + strategy_succeeds=0.0`
- No evidence of valid gameplay in entire 11.1MB log file
- Game boards remain nearly empty after 500 steps:
  ```
  Steps = 500 | State = failed | Score = 0
  ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  .‚îÇ  .‚îÇ  4‚îÇ  .‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ  .‚îÇ  .‚îÇ  .‚îÇ  .‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ  .‚îÇ  .‚îÇ  .‚îÇ  2‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
  ```
- No instances of "üéØ Max tile:" message (tile-based rewards never triggered)
- Training completed successfully but model learned nothing

**Root Cause Analysis:**

**1. Frozen Game State Loop**

Looking at `gpt_2048_rl.py:168-189`, the `do_action()` method only spawns new tiles when the board changes:

```python
def do_action(self, key: str) -> None:
    # ...
    new_board, gain, changed = mover(self._board)
    if changed:
        self._board = new_board
        self._score += gain
        self._add_random_tile()  # Only spawns if board changed!
    self._update_state_after_change()
```

**The problem:** If a strategy returns a move that doesn't change the board (e.g., moving left when all tiles are already on the left edge), then:
- `changed = False`
- No new tile spawns
- Board state remains frozen
- Strategy continues returning the same invalid move
- Loop continues until MAX_STEPS=500 timeout
- Score remains 0 (no merges occurred)

**2. Local Optimum at +2.0 Reward**

The model discovered it can reliably achieve +2.0 reward without actually playing:
- Write syntactically valid function: `function_works` = +1.0
- Don't use forbidden imports: `no_cheating` = +1.0
- Return valid move string in test: doesn't crash, gets through validation
- Game execution: frozen state, hits timeout, max_tile=4, gets 0.0 from `strategy_succeeds`
- **Total: +2.0** (stable, safe, no risk of crashes)

This is more profitable than trying complex strategies that might crash (-3.0 penalty) or timeout (-1.0 penalty).

**3. Reward Structure Has No Gradient for Early Learning**

Looking at `gpt_2048_rl.py:541-553`:

```python
if max_tile >= 8:
    level = int(math.log2(max_tile)) - 2
    tile_reward = float(level)
    scores.append(tile_reward)
else:
    scores.append(0.0)  # Tiles 2 and 4 get ZERO reward!
```

The strategies never reach tile 8, so they get **0.0** from `strategy_succeeds`. There's no gradient to climb:
- Tile 2 ‚Üí 0.0 reward
- Tile 4 ‚Üí 0.0 reward
- Tile 8 ‚Üí 1.0 reward (unreachable jump)

The model needs to randomly stumble upon a strategy that reaches tile 8 before it gets any positive reinforcement for gameplay. With frozen board states, this never happens.

**4. Evidence from Training Logs**

Early training rewards show the model was exploring but getting penalized:
```
Step 1: reward = 1.4375
Step 2: reward = -1.9375
Step 3: reward = 1.25
Step 4: reward = -5.0
...
```

By the end, it converged to the safe +2.0 local optimum:
```
Step 995-1000: reward = 2.0 (constant)
reward_std = 0.0
frac_reward_zero_std = 1.0
```

**Root Cause Summary:**
The combination of (1) frozen board states from invalid moves, (2) a safe +2.0 reward for do-nothing strategies, and (3) zero reward for tiles < 8 creates a learning environment where the model is incentivized to give up and output simple valid code rather than try to play the game.

**Solution Recommendations:**

**1. Add Reward Shaping for Small Progress**

```python
# Replace lines 541-553 in strategy_succeeds()
max_tile = max(max(row) for row in game.board())

# Reward ALL progress, not just tile >= 8
if max_tile >= 2:
    # Logarithmic scale: 2->0.5, 4->1.0, 8->1.5, 16->2.0, ...
    tile_reward = math.log2(max_tile) - 1.0
    scores.append(tile_reward)
else:
    scores.append(0.0)

# OR reward score directly (encourages merges)
score_reward = min(5.0, game.score() / 100)  # Any score gets positive reward
scores.append(score_reward)
```

**2. Penalize Frozen Board States**

```python
# Add to _execute_strategy_worker() before line 304
consecutive_frozen = 0
last_board = None

while game.state() == "ongoing":
    if steps >= MAX_STEPS:
        result_queue.put((steps, "failed"))
        return

    current_board = [row[:] for row in game.board()]
    action = strategy(current_board)

    # Detect frozen state
    if current_board == last_board:
        consecutive_frozen += 1
        if consecutive_frozen >= 10:  # 10 moves without change
            result_queue.put((steps, "failed"))
            return
    else:
        consecutive_frozen = 0

    last_board = current_board
    steps += 1
    game.do_action(action)
```

Then in `strategy_succeeds()`:
```python
# Add after line 531
if steps < 20 and game_state == "failed":
    # Failed very quickly = frozen state or bad strategy
    scores.append(-2.0)  # Penalize frozen states
    continue
```

**3. Rebalance Penalties**

The `-3.0` penalty for invalid moves is too harsh compared to `+2.0` for valid do-nothing code. This discourages exploration.

```python
# Reduce invalid move penalty
if test_move not in ["W", "A", "S", "D"]:
    scores.append(-1.0)  # Was -3.0, less harsh
    continue
```

**4. Add Move Validity Bonus**

```python
# Add after successful game execution
if steps >= 10:  # Made at least 10 valid moves
    scores.append(base_reward + 0.5)  # Small bonus for being playable
else:
    scores.append(base_reward - 0.5)  # Penalty for frozen/bad strategies
```

**5. Simplify Initial Task** (optional but recommended)

```python
# Reduce board size for faster learning
game = GameBoard(size=3, seed=seed, target=128)  # Was size=4, target=2048

# OR reward intermediate milestones
if max_tile >= 32:
    scores.append(10.0)  # Celebrate reaching 32!
```

**6. Enable complexity_reward**

Uncomment line 749 in `gpt_2048_rl.py` to enable the `complexity_reward()` function that prevents trivial single-move strategies.

**Key Insight:** The model technically succeeded at its learned objective: "generate syntactically valid code that doesn't crash." It just never learned that the code should also *play the game*. The reward structure needs to incentivize incremental gameplay progress, not just code validity.

**Files Changed:**
- `gpt_2048_rl.py` line 874: Enabled `complexity_reward` function
- `gpt_2048_rl.py` lines 307-347: Added frozen board state detection in `_execute_strategy_worker`
- `gpt_2048_rl.py` lines 419-425: Added frozen state penalty (-2.0)
- `gpt_2048_rl.py` lines 434-451: Added small rewards for tiles 2-4 (0.2, 0.5)
- `gpt_2048_rl.py` lines 409-413, 685-701, 735-740: Rebalanced penalties (less harsh)
- `BUG19_FIX_SUMMARY.md`: Created comprehensive documentation (new file)

**Solution Summary:**

The fix combines THREE mechanisms to prevent local minima:

1. **Enabled `complexity_reward`**: Creates -4.0 penalty for trivial strategies
2. **Small early tile rewards**: +0.2 (tile 2), +0.5 (tile 4) for gradient
3. **Frozen state detection**: Catches stuck strategies within 10 moves instead of 500

**The Math (prevents "return W" collapse):**
- `return "W"` + tile 4: +1 +1 -4 +0.5 = **-1.5** (unprofitable)
- Smart strategy + tile 8: +1 +1 +1 +1.0 = **+4.0** (profitable)

Even with early tile rewards, trivial strategies remain unprofitable due to complexity penalty dominating. This encourages the model to write complex, multi-directional strategies.

**Expected Results:**
- Steps 1-100: Variety of rewards, frozen state detection active
- Steps 100-500: Tile 8-16 regularly, rewards climb to +3 to +5
- Steps 500-1000: Tile 32-128, rewards climb to +7 to +11
- Frozen state frequency should decrease over time

**Implemented:**
- ‚úÖ Solution 1: Add reward shaping with safeguards (complexity penalty dominates)
- ‚úÖ Solution 2: Frozen board state detection and penalties
- ‚úÖ Solution 3: Rebalanced penalties (less harsh to encourage exploration)

---

## 20. Poor GPU Utilization - CPU Bottleneck (PERFORMANCE)

**Problem:** Training runs with very low GPU utilization (~30W power, 10% of TDP) and frequent idle periods. GPU downclocks from 2550 MHz to 2390 MHz due to spiky workload pattern. Training is CPU-bottlenecked by reward computation.

**Symptoms:**
- GPU clock speed: Starts at 2550 MHz, drops to 2390 MHz and stays there (6% perf loss)
- GPU power usage: ~30W average (should be 150-200W for compute workloads)
- GPU utilization: 80-100% with frequent drops to 0% (vertical gaps in graph)
- GPU temperature: Drops from 70¬∞C to 53¬∞C (low temp = not working hard)
- Training efficiency: Only 71% (GPU idle 29% of the time during reward computation)

**Root Cause:**

The training loop alternates between GPU-bound generation and CPU-bound reward computation:

```
[GPU: Generate completions] ‚Üí [CPU: Compute rewards] ‚Üí [GPU: Gradients] ‚Üí repeat
       ~15 seconds                   ~6-7 seconds            ~1 second
     (GPU busy)                    (GPU IDLE)            (GPU busy)
```

With `per_device_train_batch_size=8` and `num_generations=4`, only 32 completions are generated per step. This creates:
1. Short GPU burst (15s) followed by long CPU wait (6s)
2. GPU sees spiky workload and downclocks to save power
3. 30-40% of each step wasted with GPU idle
4. Low power usage (~30W vs 150-200W capable)

**Why the GPU Downclocks:**

Modern GPUs dynamically adjust clock speed based on workload patterns:
- **Sustained heavy load** ‚Üí Stay boosted at max clocks (2550 MHz)
- **Spiky/bursty load** ‚Üí Downclock to save power (2390 MHz)

The current batch size is too small to keep the GPU busy long enough to justify staying boosted.

**Solution 1: Increase Batch Size**

```python
# Line 730 in gpt_2048_rl.py
per_device_train_batch_size=16,  # Was 8, doubled
num_generations=4,                # Keep at 4
# Total: 64 completions per step instead of 32
```

**Impact:**
- GPU generation time: ~30s (doubled)
- Reward computation time: ~12s (but same proportion)
- Training efficiency: 71% ‚Üí 83% (12% improvement)
- Clock speed: Should stay at 2550 MHz (no downclock)
- Power usage: Should increase to 100-150W sustained
- Step time: ~36s vs ~21s (1.7x slower per step, but 2x more work per step)
- **Net result: ~1.2x faster training overall**

**Solution 2: Reduce Timeout**

```python
# Line 317 in gpt_2048_rl.py
def execute_strategy(strategy: Callable, game: GameBoard, timeout: float = 1.0):
    # Was 2.0, reduced to 1.0
```

**Impact:**
- With 7,735 timeouts in previous run: Saves 7,735 √ó 1s = **2 hours**
- Faster rejection of bad strategies (infinite loops, frozen states)
- Less time wasted on clearly failing code

**Solution 3: Parallel Reward Computation** (IMPLEMENTED)

Reward computation now supports parallel execution using `ProcessPoolExecutor`. Controlled by flags at the top of the file:

```python
# Lines 52-53 in gpt_2048_rl.py
ENABLE_PARALLEL_REWARDS = True  # Set to False to disable
PARALLEL_WORKERS = 8            # Number of parallel workers
```

**How it works:**
- Phase 1: Fast validation (extract, check syntax, check modules) runs sequentially
- Phase 2: Expensive game execution runs in parallel (8 games at once)
- Worker function `_run_game_worker()` handles complete game evaluation
- Results are merged back and printed in order

**Expected impact:**
- Reward computation: 6s ‚Üí 1-2s (3-6x faster)
- Training efficiency: 71% ‚Üí 94%
- Step time: ~36s ‚Üí ~25s (even with doubled batch size)
- **Combined with batch size increase: ~2x faster training overall**

**Files Changed:**
- `gpt_2048_rl.py` line 24: Added `from concurrent.futures import ProcessPoolExecutor`
- `gpt_2048_rl.py` lines 45-53: Added configuration flags `ENABLE_PARALLEL_REWARDS` and `PARALLEL_WORKERS`
- `gpt_2048_rl.py` lines 371-424: Added `_run_game_worker()` function for parallel game execution
- `gpt_2048_rl.py` lines 518-686: Refactored `strategy_succeeds()` to support parallel execution
- `gpt_2048_rl.py` line 730: Changed `per_device_train_batch_size=8` ‚Üí `16`
- `gpt_2048_rl.py` line 317: Changed `timeout: float = 2.0` ‚Üí `1.0`
- `gpt_2048_rl.py` line 322: Updated docstring to mention 1.0s timeout
- `gpt_2048_rl.py` line 556: Updated error message "2s exceeded" ‚Üí "1s exceeded"

**Expected Results After All Three Fixes:**

Before:
- Clock speed: 2390 MHz (downclocked)
- Power: ~30W (10% of capacity)
- Utilization: 71% (frequent idle gaps)
- Step time: ~21s for 32 completions
- Timeout waste: 7,735 √ó 2s = ~4 hours per run

After (batch size + timeout + parallel):
- Clock speed: 2550 MHz (staying boosted) ‚úì
- Power: 100-150W (GPU actually working) ‚úì
- Utilization: 94%+ (minimal idle gaps) ‚úì
- Step time: ~25s for 64 completions ‚úì
- Timeout waste: 7,735 √ó 1s = ~2 hours (saved 2 hours) ‚úì
- **Overall: ~2x faster training** (1.2x from GPU + 0.6x from parallel + 2h from timeout)

**Key Insight:** The GPU was "bored" - the workload wasn't challenging enough to justify staying at max clocks. Doubling the batch size gives the GPU more work to do per step, keeping it engaged and boosted. The clock speed drop from 2550‚Üí2390 MHz was the GPU's way of saying "this isn't worth staying hot for."

---
