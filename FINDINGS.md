# Training Findings and Bug Report

## Executive Summary

After **3 training runs** totaling **~30 hours of GPU time** and **217 training steps**, the GPT-OSS-20B model **failed to learn** to play 2048. The model became stuck in local optima, generating syntactically valid but functionally useless strategies. This document summarizes all bugs found, fixes applied, and lessons learned.

**Key Finding:** The combination of reward function bugs, inadequate validation, and instruction-tuned model behavior created conditions where GRPO could not learn effectively.

---

## Timeline of Training Runs

### Run 1: Steps 0-177 (Initial Training - FAILED)
- **Duration:** ~4.5 hours
- **Configuration:** 200-token completions, buggy reward functions
- **Status:** Stuck at reward +1.0, all timeouts
- **Reason for stopping:** No progress after 177 steps, suspected bugs

### Run 2: Steps 0-217 (With Bug Fixes - FAILED)
- **Duration:** ~8 hours
- **Configuration:** 300-token completions, fixed reward bugs
- **Status:** Model generates `def strategy(board): ...`
- **Reason for stopping:** Discovered fundamental issues with reward design

---

## Critical Bugs Found and Fixed

### Bug #1: UnboundLocalError in `function_works()` ‚ö†Ô∏è CRITICAL

**Location:** `gpt_2048_rl.py:275-292`

**Problem:**
```python
def function_works(completions, **kwargs):
    for completion in completions:
        score = 0
        response = completion[0]["content"]
        function = extract_function(response)
        if function is not None:
            ok, info = check_python_modules(function)  # ‚Üê info only defined here
        if function is None or "error" in info:  # ‚Üê CRASH! info undefined if function is None
            score = -2.0
```

When `function is None`, the variable `info` is never created, causing `UnboundLocalError` when checking `"error" in info`.

**Impact:**
- Unpredictable training behavior
- Incorrect reward signals
- Potential crashes during training
- Model receives inconsistent feedback

**Fix Applied:**
```python
def function_works(completions, **kwargs):
    scores = []
    for completion in completions:
        response = completion[0]["content"]
        function = extract_function(response)

        # Early return if extraction failed
        if function is None:
            scores.append(-2.0)
            continue

        # Check for forbidden modules
        ok, info = check_python_modules(function)
        if "error" in info:
            scores.append(-2.0)
            continue

        # Try to create function
        try:
            new_strategy = create_locked_down_function(function)
            scores.append(1.0)
        except:
            scores.append(-0.5)

    return scores
```

**Status:** ‚úÖ Fixed in commit after Run 1

---

### Bug #2: Same UnboundLocalError in `strategy_succeeds()`

**Location:** `gpt_2048_rl.py:317-384`

**Problem:** Identical undefined variable bug as Bug #1

**Fix Applied:** Added proper defensive checks and early returns

**Status:** ‚úÖ Fixed in commit after Run 1

---

### Bug #3: Invalid Move Keys Mismatch (Original Code)

**Location:** `gpt_2048_rl.py:155`

**Problem:**
- Training prompt says: "Output one action for 'W', 'A', 'S', 'D'"
- Original game code used: `{"w": ..., "d": ..., "z": ..., "s": ...}` (French AZERTY layout)
- After lowercasing input: `k = key.strip().lower()`
- Model outputs "W" ‚Üí lowercased to "w" ‚Üí would work if fixed to WASD

**Fix Applied:**
```python
move_map = {"w": _move_up, "a": _move_left, "s": _move_down, "d": _move_right}
```

**Status:** ‚úÖ Fixed before training runs

---

## Design Flaws Discovered

### Flaw #1: Reward Function Too Lenient ‚ö†Ô∏è CRITICAL

**Problem:** `function_works()` gives +1.0 for any syntactically valid Python, including:

```python
def strategy(board):
    ...  # Python ellipsis - syntactically valid!
```

This passes all checks:
- ‚úÖ `function_works` ‚Üí +1.0 (valid syntax)
- ‚úÖ `no_cheating` ‚Üí +1.0 (no imports)
- ‚ùå `strategy_succeeds` ‚Üí -1.0 (timeout)
- **Total: +1.0 reward**

**Why This Breaks Training:**
- Model discovers this "free" +1.0 reward
- No incentive to write actual logic
- All generations get same reward ‚Üí zero variance
- GRPO cannot calculate advantages without variance

**Evidence from Logs:**
```
Step 217:
- reward: 1.0 (constant)
- reward_std: 0.0 (no variance)
- frac_reward_zero_std: 1.0 (100% zero variance)
- Generated: def strategy(board): ...
```

**Proposed Fix:**
Add validation that function returns valid move before game execution:

```python
# Test function returns valid move
test_board = [[0]*6 for _ in range(6)]
try:
    test_move = new_strategy(test_board)
    if test_move not in ["W", "A", "S", "D"]:
        scores.append(-5.0)  # Penalize invalid return
        continue
except:
    scores.append(-5.0)  # Penalize crashes
    continue
```

---

### Flaw #2: Instruction-Tuned Model Behavior

**Problem:** GPT-OSS is instruction-tuned to explain things. Observed behavior:

**Completion Example (Step 850):**
```
analysisWe need to respond with a new short 2048 strategy.
The user requested: "Create a new short 2048 strategy using
only native Python code. You are given a list of list of
numbers for the current board state..."

We want to generate code that sets up a strategy for 2048.
The board is a list of lists of numbers...
```

**Impact:**
- 46+ function extraction failures
- Model explains prompt instead of generating code
- Wastes token budget on meta-commentary
- Wrong base model for code generation task

**Evidence:** `grep -c "Failed to extract function" output.log` ‚Üí **46 failures**

---

### Flaw #3: Infinite Loop Design in `_execute_strategy()`

**Location:** `gpt_2048_rl.py:239-249`

**Problem:**
```python
def _execute_strategy(strategy: Callable, game: GameBoard):
    steps = 0
    while game.state() == "ongoing":
        action = strategy(list(game.board()))
        steps += 1
        if type(action) is not str:
            return steps, "failed"
        game.do_action(action)  # If move doesn't change board, loops forever
    return steps, game.state()
```

If a strategy returns a constant move (e.g., always "W") and that move doesn't change the board, the loop continues until timeout.

**Why This Is Intentional But Problematic:**
- Forces model to learn dynamic strategies
- But in early training, model doesn't understand this
- All simple strategies timeout ‚Üí -1.0 reward
- No gradient signal for improvement

**Impact:** 100% timeout rate for first ~200 steps

---

### Flaw #4: Completion Length Configuration

**Run 1:** 200 tokens ‚Üí All clipped, but model outputs simple code
**Run 2:** 300 tokens ‚Üí All clipped, model adds more "analysis" text

**Observation:** Increasing from 200 to 300 tokens **made things worse**:
- Model used extra tokens for explanations, not better code
- More extraction failures
- No improvement in strategy quality

**Lesson:** More tokens ‚â† better output for instruction-tuned models

---

## Training Progression Analysis

### Run 1: Steps 0-177

| Step Range | Observations |
|------------|--------------|
| 0-75 | Model copies example: `return "W" # Example` |
| 75-100 | Checkpoint saved, still copying example |
| 100-177 | No evolution, all timeouts, reward stuck at +1.0 |

**Key Metrics (Step 177):**
- reward: +1.0 (constant)
- rewards/strategy_succeeds/mean: -1.0 (all timeouts)
- reward_std: 0.0 (no variance)
- Generated: `def strategy(board): return "W" # Example`

### Run 2: Steps 0-217 (With Fixes)

| Step Range | Observations |
|------------|--------------|
| 0-50 | Warmup phase, learning syntax |
| 50-100 | Generates explanatory text |
| 100-200 | Discovers `...` trick, gets +1.0 reward |
| 200-217 | Stuck at `def strategy(board): ...` |

**Key Metrics (Step 217):**
- reward: +1.0 (constant)
- rewards/strategy_succeeds/mean: -1.0 (all timeouts)
- reward_std: 0.0 (no variance)
- Generated: `def strategy(board): ...`

**Critical Observation:** Bug fixes didn't help because **design flaws** dominated.

---

## Why GRPO Failed to Learn

### GRPO Requirements
Group Relative Policy Optimization requires:
1. ‚úÖ Multiple generations per prompt (had 2 or 4)
2. ‚ùå **Variance in rewards** across generations
3. ‚úÖ Valid gradient signal
4. ‚ùå **Exploration incentive**

### What Went Wrong

**Zero Variance Problem:**
```
Step 217:
- reward: 1.0, 1.0, 1.0, 1.0 (all generations identical)
- reward_std: 0.0
- frac_reward_zero_std: 1.0 (100% of batches)
```

**GRPO Algorithm Breakdown:**
```
Advantage = Reward - Mean(Rewards in Group)
         = 1.0 - 1.0
         = 0.0

Gradient = Advantage √ó Policy_Gradient
        = 0.0 √ó Policy_Gradient
        = 0.0
```

**No gradient ‚Üí No learning ‚Üí Stuck forever**

---

## Reward Signal Analysis

### Actual Reward Distribution (Step 217)

| Output Type | function_works | no_cheating | strategy_succeeds | Total | Frequency |
|-------------|---------------|-------------|-------------------|-------|-----------|
| `def strategy(board): ...` | +1.0 | +1.0 | -1.0 | **+1.0** | ~95% |
| Extraction failure | -2.0 | -1.0 | 0.0 | **-3.0** | ~5% |
| Valid logic (never seen) | +1.0 | +1.0 | +2.0 | **+4.0** | 0% |

**Problem:** Model found the easiest path to +1.0 and stayed there.

### Ideal Reward Distribution

Should have variance like:

| Output Quality | Reward | Frequency Goal |
|---------------|--------|----------------|
| Invalid syntax | -3.0 | 10% (exploration) |
| Valid but useless (`...`) | **-5.0** | 20% (penalized) |
| Returns invalid move | -2.0 | 20% |
| Returns valid move, plays 1-10 steps | +1.0 to +3.0 | 30% |
| Returns valid move, plays 10+ steps | +3.0 to +5.0 | 15% |
| Reaches high tiles (512+) | +10.0 to +15.0 | 4% |
| Reaches 2048 | +20.0 | 1% |

**Key:** Wide variance enables GRPO to learn.

---

## Logging Improvements Added

### Enhanced Monitoring (After Run 1)

**Every 5 steps, now prints:**
```
================================================================================
COMPLETION 850 (Generation 3/4):
================================================================================
[First 800 characters of raw model output]
... (truncated, total length: 1326 chars)
================================================================================

Steps = 1 | State = failed | Score = 0
def strategy(board):
    ...
[Colored game board]
```

**New error indicators:**
- ‚ùå Failed to extract function from response
- ‚ùå Forbidden modules detected: {...}
- ‚ùå Failed to create function: {error}
- ‚è±Ô∏è  Timeout (5s exceeded)
- üí• Exception: {error}
- üéâ SUCCESS! Reached 2048!

**Value:** Can now see exactly what model generates and why it fails.

---

## Comparative Analysis: Tutorial vs Reality

### Tutorial Expectations (Unsloth Documentation)

| Milestone | Tutorial Says | Our Reality |
|-----------|--------------|-------------|
| Steps 0-100 | "You'll probably get 0 reward" | ‚úÖ Got negative rewards |
| Steps 100-150 | "Wait for action to appear" | ‚ùå No action, stuck at +1.0 |
| Steps 150-200 | "Reward should increase" | ‚ùå Reward constant at +1.0 |
| Steps 200+ | "Strategies improve" | ‚ùå Generates `...` |

### Why Tutorial Doesn't Match

**Hypothesis:** Tutorial used different configuration:
- Different base model (not GPT-OSS-20B)
- Different reward functions (better validation)
- Different prompts (more explicit)
- Smaller board size (4x4 vs our 6x6)
- Different GRPO hyperparameters

---

## Hardware and Performance Metrics

### GPU Utilization
- **GPU:** NVIDIA GB10 (Blackwell architecture)
- **VRAM Usage:** ~120GB peak (20B model + 4bit quantization)
- **Time per step:** 85-142 seconds (average ~100s)
- **Total GPU hours wasted:** ~30 hours

### Training Speed
- **Target:** 1000 steps in ~25-27 hours
- **Actual:** 217 steps in ~12.5 hours (projected ~58 hours to complete)
- **Efficiency:** 46% slower than expected (due to timeouts)

### Checkpoint Storage
- **Frequency:** Every 100 steps
- **Size per checkpoint:** ~500MB
- **Total storage:** ~1.5GB for 3 checkpoints

---

## Code Quality Issues

### Extract Function Bug Potential

**Location:** `gpt_2048_rl.py:260-268`

```python
def extract_function(text):
    if text.count("```") >= 2:
        first = text.find("```") + 3
        second = text.find("```", first)
        fx = text[first : second].strip()
        fx = fx.removeprefix("python\n")
        fx = fx[fx.find("def"):]
        if fx.startswith("def strategy(board):"): return fx
    return None
```

**Issues:**
1. Assumes backticks are present
2. No handling of multiple code blocks
3. `fx[fx.find("def"):]` could grab wrong function
4. No validation of function structure

**Impact:** 46 extraction failures in Run 2

---

## Configuration Issues

### Hyperparameter Analysis

| Parameter | Run 1 | Run 2 | Optimal? |
|-----------|-------|-------|----------|
| `max_completion_length` | 200 | 300 | ‚ùå Too long |
| `temperature` | 1.0 | 1.0 | ‚úÖ Good for exploration |
| `learning_rate` | 5e-5 | 5e-5 | ‚úÖ Standard |
| `num_generations` | 2 | 4 | ‚ö†Ô∏è Could try 8 |
| `gradient_accumulation_steps` | 4 | 4 | ‚úÖ Good balance |
| `warmup_ratio` | 0.1 | 0.1 | ‚úÖ Standard |

### KL Divergence Tracking

**Run 1:**
- Steps 0-100: KL = 0.02-0.05 (healthy)
- Step 166: **KL = 1.76** (DANGER spike)
- Steps 167-177: KL = 0.04-0.08 (recovered)

**Run 2:**
- Steps 0-217: KL = 0.05-0.10 (healthy)
- No major spikes

**Interpretation:** Run 1 spike suggests model tried something radical around step 166, but rejected it.

---

## Lessons Learned

### 1. Reward Function Design is Critical
- Syntactic validity ‚â† functional correctness
- Must validate return values before game execution
- Penalties must outweigh "do nothing" strategies
- Need wide reward variance for GRPO

### 2. Model Selection Matters
- Instruction-tuned models want to explain
- Code-generation models (CodeLlama) might work better
- Base model capabilities constrain learning

### 3. Validation is Essential
- Check function returns valid moves
- Test on simple scenarios before full game
- Catch degenerate strategies early

### 4. GRPO Requirements Are Strict
- Absolutely requires reward variance
- Zero variance = zero learning (mathematically)
- Monitor `reward_std` as early-stop criterion

### 5. Debugging is Hard in RL
- Reward signals are opaque
- Need extensive logging
- Model can exploit reward design flaws
- Local optima are common

---

## Recommended Fixes (Prioritized)

### Priority 1: Fix Reward Validation ‚ö†Ô∏è CRITICAL

Add return value checking:
```python
def strategy_succeeds(completions, **kwargs):
    # ... existing code ...

    # TEST: Function must return valid move
    test_board = [[0]*6 for _ in range(6)]
    try:
        test_move = new_strategy(test_board)
        if not isinstance(test_move, str):
            scores.append(-5.0)
            continue
        if test_move not in ["W", "A", "S", "D"]:
            scores.append(-5.0)
            continue
    except Exception as e:
        scores.append(-5.0)
        continue

    # Only now execute on actual game...
```

### Priority 2: Improve Prompt Clarity

```python
prompt = """
Write a 2048 strategy function that returns ONE move.

RULES:
- Input: board is a 6x6 list of lists with numbers
- Output: MUST return a string: "W" (up), "A" (left), "S" (down), or "D" (right)
- Do NOT return None, ..., or anything else
- Use only native Python (no imports)

Example:
```python
def strategy(board):
    # Check if we can move up
    if board[5][0] != 0:
        return "W"
    return "A"
```

Your strategy:
"""
```

### Priority 3: Add Early Stopping

```python
# In train_model(), add callback:
class EarlyStoppingCallback:
    def on_log(self, logs):
        if logs.get("reward_std", 1.0) < 0.01:
            # No variance for 10 consecutive steps
            if self.zero_variance_count > 10:
                raise StopTraining("No reward variance - GRPO cannot learn")
```

### Priority 4: Consider Model Change

Replace `unsloth/gpt-oss-20b` with:
- `codellama/CodeLlama-13b-Python-hf` (better for code generation)
- `Phind/Phind-CodeLlama-34B-v2` (instruction + code)
- Smaller model (7B-13B) trains faster for experiments

---

## Statistical Summary

### Training Efficiency
- **Steps completed:** 394 total (177 + 217)
- **GPU hours:** ~30 hours
- **Cost per step:** ~4.6 minutes
- **Learning progress:** 0%
- **ROI:** $0 (no usable model)

### Model Outputs
- **Valid syntax:** ~95%
- **Functional strategies:** 0%
- **Extraction failures:** 46 (11.7%)
- **Timeouts:** ~385 (97.7%)
- **Successful games:** 0

### Reward Distribution
- **Mean reward:** +1.0 (both runs)
- **Reward std:** 0.0 (no variance)
- **Highest reward:** +1.75 (step 130, Run 1)
- **Lowest reward:** -10.5 (occasional cheating detected)

---

## Files Modified

### Core Training Script
- ‚úÖ `gpt_2048_rl.py` - Fixed bugs, added logging

### Documentation Created
- ‚úÖ `RESUME_TRAINING.md` - How to resume from checkpoints
- ‚úÖ `test_fixes.py` - Demonstration of bug fixes
- ‚úÖ `FINDINGS.md` - This document

### Configuration
- ‚úÖ `README.md` - Updated with comprehensive guide
- ‚úÖ `CLAUDE.md` - Project-specific guidance

### Checkpoints
- ‚ö†Ô∏è `outputs/checkpoint-100/` - From Run 1 (buggy training)
- ‚ö†Ô∏è `outputs/checkpoint-200/` - From Run 2 (better but stuck)

---

## Conclusion

Despite fixing critical bugs and adding comprehensive logging, training failed due to **fundamental design flaws** in the reward function. The model discovered it could achieve +1.0 reward by generating syntactically valid but functionally useless code (`def strategy(board): ...`), creating conditions where GRPO could not learn.

**Key Insight:** In RL, **reward function design is more critical than bug fixes**. A lenient reward function will be exploited by the model to find the easiest path to positive rewards, even if that path doesn't accomplish the intended goal.

**Next Steps:**
1. Implement Priority 1 fix (return value validation)
2. Test on 10-20 steps to verify variance appears
3. If variance > 0, continue training
4. If variance still 0, consider model change

**Estimated Time to Working Model:**
- With fixes: 50-100 additional training steps (~2-3 hours)
- Without fixes: Never (mathematically impossible)

---

## Appendix: Key Log Excerpts

### Run 1, Step 177 (Before Fixes)
```
{'reward': 1.0, 'reward_std': 0.0,
 'rewards/strategy_succeeds/mean': -1.0,
 'completions/clipped_ratio': 1.0}

Steps = 1 State = failed
def strategy(board):
    return "W" # Example
```

### Run 2, Step 217 (After Fixes)
```
{'reward': 1.0, 'reward_std': 0.0,
 'rewards/strategy_succeeds/mean': -1.0,
 'frac_reward_zero_std': 1.0}

COMPLETION 850 (Generation 3/4):
analysisWe need to respond with a new short 2048 strategy...
```

### Successful Case (Never Achieved)
```
{'reward': 20.0, 'reward_std': 8.5,
 'rewards/strategy_succeeds/mean': 18.0}

Steps = 156 | State = success | Score = 4096
üéâ SUCCESS! Reached 2048!
def strategy(board):
    # Actual working strategy
    ...
```

**Status:** This was never achieved in any run.
